{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob.tokenizers import WordTokenizer as WT\n",
    "from scipy.sparse import lil_matrix as lm\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "from time import time\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caleb/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (22,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"FOX.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetGenerator:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.rawtweets = df\n",
    "        self.regexReplace()\n",
    "        self.handleQuotes()\n",
    "        self.splitWords()\n",
    "        self.buildMat()\n",
    "\n",
    "    def regexReplace(self):\n",
    "        \n",
    "        tweets = self.rawtweets.str.replace(\"â€¦\", \"...\").str.replace(r\"https?.*\", \"\", regex=True).str.replace(\"&amp;\", \"&\")\n",
    "        tweets = tweets.str.replace(\"w/\", \"with \").str.replace(r\".*@.*\", \"\", regex=True).str.replace(r\"[^A-Za-z0-9:;,\\.&\\'\\\"\\-!\\(\\)\\[\\]]\", \" \", regex=True)\n",
    "        tweets = tweets.str.replace(r\"^[^a-z]*:([^a-z]*:)?[^A-Za-z0-9]\", \"\", regex=True).str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        tweets = tweets.str.replace(\"::\", \":\").str.replace(r\"^(\\s|\\t)*\", \"\", regex=True).str.replace(r\"(\\s|\\t)*$\", \"\", regex=True)        \n",
    "        \n",
    "        self.cleanedTweets = tweets.drop(np.arange(tweets.shape[0])[np.array([len(i) == 0 for i in tweets.values])]).reset_index()[\"tweet\"]\n",
    "        self.size = self.cleanedTweets.shape[0]\n",
    "        \n",
    "    def handleQuotes(self):\n",
    "        quoteLists = self.cleanedTweets.str.findall(r\"\\s\\'[^']*\\'\").values\n",
    "        newSeries = []\n",
    "        count = 0 \n",
    "        for i in quoteLists:\n",
    "            if len(i) < 1:\n",
    "                newSeries += [self.cleanedTweets[count]]\n",
    "                count += 1\n",
    "                continue\n",
    "            for j in i:\n",
    "                temp = self.cleanedTweets[count].replace(j, \" \" + j[1:].replace(\"\\'\", \"@\").replace(\" \", \"_\")) \n",
    "            newSeries += [temp]\n",
    "            count += 1\n",
    "        self.newSeries = newSeries\n",
    "        \n",
    "    def splitWords(self):\n",
    "        \n",
    "        self.tokenized = []        \n",
    "        master_blob = TextBlob(\"%\".join(self.newSeries)).tags\n",
    "        \n",
    "        blobs = []\n",
    "        count = 0\n",
    "        while count < len(master_blob):\n",
    "            temp = []\n",
    "            while (count < len(master_blob)) and (master_blob[count][0] != \"%\"):\n",
    "                temp += [master_blob[count]]\n",
    "                count += 1\n",
    "            blobs += [temp]\n",
    "            count += 1\n",
    "\n",
    "        count = 0\n",
    "        for text in self.newSeries:\n",
    "            temp = [\"^*^$TART\"] + [i[0] + \"^*^\" + i[1] for i in blobs[count]] + [\"^*^$TOP\"]\n",
    "            count += 1\n",
    "            for i in range(1, len(temp)-2):\n",
    "                if temp[i][0] == \"@\":\n",
    "                    temp = temp[:i] + [\"\\'\" + temp[i+1].replace(\"_\", \" \").replace(\"^*^\", \"\\'^*^\")] + temp[i+3:]\n",
    "                    break\n",
    "            self.tokenized += [temp]\n",
    "        \n",
    "        self.words = set()\n",
    "        pairs, trios = [], set()\n",
    "        for sentence in self.tokenized:\n",
    "            n = len(sentence)\n",
    "            self.words.add(sentence[0])\n",
    "            for i in range(n-1):\n",
    "                pairs += [(sentence[i], sentence[i+1])]\n",
    "                try:\n",
    "                    trios.add((sentence[i], sentence[i+2]))\n",
    "                except:\n",
    "                    n = len(sentence)\n",
    "                self.words.add(sentence[i+1])\n",
    "        counter = collections.Counter(pairs)\n",
    "        self.trios = trios\n",
    "        self.most_common = set(counter.most_common())\n",
    "    \n",
    "    def buildMat(self):\n",
    "        \n",
    "        self.n = len(self.words)\n",
    "        self.word_map = dict(zip(list(self.words), np.arange(self.n)))\n",
    "        mat = lm((self.n,self.n))\n",
    "        \n",
    "        for i in self.most_common:\n",
    "            words, freq = i\n",
    "            left, right = words\n",
    "            mat[self.word_map[left], self.word_map[right]] = freq\n",
    "        self.mat = normalize(mat, norm='l1', axis=1)\n",
    "        \n",
    "    def makeTweets(self, num_tweets=1, min_words=4):\n",
    "        \n",
    "        first_ind = self.word_map[\"^*^$TART\"]\n",
    "        final_ind = self.word_map[\"^*^$TOP\"]\n",
    "        self.number_map = inv_map = {v: w for w, v in self.word_map.items()}\n",
    "        \n",
    "        all_headlines = []\n",
    "        \n",
    "        while 1:\n",
    "            chance = chances\n",
    "            failed = False\n",
    "            headline = []\n",
    "            first = True\n",
    "            while 1:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    last_ind = np.random.choice(np.arange(self.n), p=self.mat.getrow(first_ind).toarray()[0])\n",
    "                else:\n",
    "                    temp = last_ind\n",
    "                    last_ind = np.random.choice(np.arange(self.n), p=self.mat.getrow(last_ind).toarray()[0])\n",
    "                    \n",
    "                next_word = self.number_map[last_ind]\n",
    "                \n",
    "                if last_ind == final_ind:\n",
    "                    if len(headline) <= min_words:\n",
    "                        failed = True                      \n",
    "                    break\n",
    "                else:\n",
    "                    headline += [next_word]\n",
    "            if failed:\n",
    "                continue\n",
    "            \n",
    "            all_headlines += [\" \".join([i.split(\"^*^\")[0] for i in headline])]\n",
    "            if len(all_headlines) == num_tweets:\n",
    "                break\n",
    "        return all_headlines\n",
    "        \n",
    "gf = TweetGenerator(df.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Bush 's Lawyer for Obama s team at Radio Address Liquidity Upgrades Electronics Inc Announces Significant Sanctions Against Infomercial Airs First Quarter\",\n",
       " \"JSE All-Share Index Investors Wait for doctors by angry mob attack at a bit of life with Usama Bin Laden 'May Be Dead' Pakistan injuring 14 Billion\",\n",
       " 'Upskirt photo of the Dogs The Year Ago President Obama Botches Surgery',\n",
       " 'AARP Endorsement of these real Hollywood',\n",
       " 'Israeli archaeologist says she was shocked that it scrambled in court to CHP Redding California.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf.makeTweets(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
